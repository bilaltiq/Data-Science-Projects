count(word, sort = TRUE)
?unnest_tokens
#| label: load-packages
#| include: false
library(mosaic)
library(tidyverse)
library(tidytext)
library(mdsr)
library(Lahman)
library(kableExtra)
library(ggplot2)
library(stringr)
# add other packages here!
#| label: load-packages
#| include: false
library(mosaic)
library(tidyverse)
library(tidytext)
library(tidyr)
library(mdsr)
library(Lahman)
library(kableExtra)
library(ggplot2)
library(stringr)
# add other packages here!
#| fig-width: 8
#| fig-height: 8
data(stop_words)
frequencyTweets <- tweets %>%
unnest_tokens(output = word, input = text) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE)
View(frequencyTweets)
#| fig-width: 8
#| fig-height: 8
data(stop_words)
frequencyTweets <- tweets %>%
unnest_tokens(output = word, input = text) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE)
#We also don't want words like https and .co so let's filter them out
frequencyTweets %>%
filter(word != "https") %>%
filter(word != "t.co")
#| fig-width: 8
#| fig-height: 8
data(stop_words)
frequencyTweets <- tweets %>%
unnest_tokens(output = word, input = text) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE)
#We also don't want words like https and .co so let's filter them out
frequencyTweets %>%
slice(1:50) %>%
filter(word != "https") %>%
filter(word != "t.co")
set.seed(231)
mypal <- brewer.pal(10, "Paired")
install.packages("wordcloud")
#| label: load-packages
#| include: false
library(mosaic)
library(tidyverse)
library(tidytext)
library(tidyr)
library(mdsr)
library(Lahman)
library(kableExtra)
library(ggplot2)
library(stringr)
library(wordcloud)
# add other packages here!
#| label: load-packages
#| include: false
library(mosaic)
library(tidyverse)
library(tidytext)
library(tidyr)
library(mdsr)
library(Lahman)
library(kableExtra)
library(ggplot2)
library(stringr)
library(wordcloud)
# add other packages here!
#| message: false
hof <- Batting %>%
group_by(playerID) %>%
inner_join(HallOfFame, by = "playerID") %>%
filter(inducted == "Y" & votedBy == "BBWAA") %>%
summarize(tH = sum(H), tHR = sum(HR), tRBI = sum(RBI), tSB = sum(SB)) %>%
filter(tH > 1000)
kable(hof)
set.seed(100)
clustering_prep_hof <- hof %>%
select("tH","tHR") %>%
drop_na()
glimpse(clustering_prep_hof)
set.seed(231) #Reproducibility
clustering_hof <- clustering_prep_hof %>%
select("tH", "tHR") %>%
kmeans(centers = 4, nstart = 20)
clustering_hof$cluster
clustering_hof$centers
hof <- hof %>%
mutate(clusters2 = factor(clustering_hof$cluster))
ggplot(data = hof, aes(x = tH, y = tHR)) +
geom_point(aes(color = clusters2)) +
coord_fixed() +
geom_point(data = data.frame(clustering_hof$centers),
aes(x = tH, y = tHR),
pch = "o", size = 8, color = "gray") +
labs(x = "Number of Career Hits",
y = "Number of Home Runs",
color = "Cluster assignment") + theme_minimal()
# the .rda file is also provided if this website ever breaks
load(url("http://varianceexplained.org/files/trump_tweets_df.rda"))
trump_tweets_df2 <- trump_tweets_df %>%
filter(screenName == "realDonaldTrump")
tweets <- trump_tweets_df2 %>%
select(text,created,statusSource)
sourcesTweets <- tweets %>%
group_by(statusSource) %>%
summarise(Number_Tweets = n())
kable(sourcesTweets, booktabs = TRUE)
#| fig-width: 8
#| fig-height: 8
data(stop_words)
frequencyTweets <- tweets %>%
unnest_tokens(output = word, input = text) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE)
#We also don't want words like https and .co so let's filter them out
frequencyTweets %>%
slice(1:50) %>%
filter(word != "https") %>%
filter(word != "t.co")
set.seed(231)
mypal <- brewer.pal(10, "Paired")
frequencyTweets %>%
with(wordcloud(words = word,
freq = n,
min.freq = 20,
max.freq = 200,
random.order = TRUE,
scale = c(2,0.3),
rot.per = 0.15,
colors = mypal,
family = "serif"))
#| fig-width: 8
#| fig-height: 8
data(stop_words)
frequencyTweets <- tweets %>%
unnest_tokens(output = word, input = text) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE)
#We also don't want words like https and .co so let's filter them out
frequencyTweets %>%
slice(1:50) %>%
filter(word != "https") %>%
filter(word != "t.co")
set.seed(231)
mypal <- brewer.pal(10, "Paired")
frequencyTweets %>%
with(wordcloud(words = word,
freq = n,
min.freq = 20,
max.freq = 50,
random.order = TRUE,
scale = c(2,0.3),
rot.per = 0.15,
colors = mypal,
family = "serif"))
#| fig-width: 8
#| fig-height: 8
data(stop_words)
frequencyTweets <- tweets %>%
unnest_tokens(output = word, input = text) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE)
#We also don't want words like https and .co so let's filter them out
frequencyTweets %>%
slice(1:50) %>%
filter(word != "https") %>%
filter(word != "t.co")
set.seed(231)
mypal <- brewer.pal(10, "Paired")
frequencyTweets %>%
with(wordcloud(words = word,
freq = n,
min.freq = 20,
random.order = TRUE,
scale = c(2,0.3),
rot.per = 0.15,
colors = mypal,
family = "serif"))
#| fig-width: 8
#| fig-height: 8
data(stop_words)
frequencyTweets <- tweets %>%
unnest_tokens(output = word, input = text) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE)
#We also don't want words like https and .co so let's filter them out
set.seed(231)
mypal <- brewer.pal(10, "Paired")
frequencyTweets %>%
slice(1:50) %>%
filter(word != "https") %>%
filter(word != "t.co") %>%
with(wordcloud(words = word,
freq = n,
min.freq = 20,
random.order = TRUE,
scale = c(2,0.3),
rot.per = 0.15,
colors = mypal,
family = "serif"))
#| fig-width: 8
#| fig-height: 8
data(stop_words)
frequencyTweets <- tweets %>%
unnest_tokens(output = word, input = text) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE)
#We also don't want words like https and .co so let's filter them out
set.seed(231)
mypal <- brewer.pal(10, "Paired")
frequencyTweets %>%
slice(1:50) %>%
filter(word != "https") %>%
filter(word != "t.co") %>%
with(wordcloud(words = word,
freq = n,
min.freq = 20,
random.order = TRUE,
scale = c(3,0.3),
rot.per = 0.15,
colors = mypal,
family = "serif"))
#| fig-width: 8
#| fig-height: 8
data(stop_words)
frequencyTweets <- tweets %>%
unnest_tokens(output = word, input = text) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE)
#We also don't want words like https and .co so let's filter them out
set.seed(231)
mypal <- brewer.pal(10, "Paired")
frequencyTweets %>%
slice(1:50) %>%
filter(word != "https") %>%
filter(word != "t.co") %>%
with(wordcloud(words = word,
freq = n,
min.freq = 20,
random.order = TRUE,
scale = c(4,0.3),
rot.per = 0.15,
colors = mypal,
family = "serif"))
View(tweets)
#| eval: true # remove eval: false when working on this!
tweets <- tweets %>%
extract(col = statusSource, into = "source",
regex = "Twitter for (.*)<",
remove = FALSE) %>%
filter(source %in% c("Android", "iPhone"))
#| eval: true # remove eval: false when working on this!
tweets <- tweets %>%
extract(col = statusSource, into = "source",
regex = "Twitter for (.*)<",
remove = FALSE) %>%
filter(source %in% c("Android", "iPhone"))
glimpse(tweets)
#| eval: true # remove eval: false when working on this!
tweets <- tweets %>%
extract(col = statusSource, into = "source",
regex = "Twitter for (.*)<",
remove = FALSE) %>%
filter(source %in% c("Android", "iPhone"))
kable(tweets)
#| eval: true # remove eval: false when working on this!
tweets <- tweets %>%
extract(col = statusSource, into = "source",
regex = "Twitter for (.*)<",
remove = FALSE) %>%
filter(source %in% c("Android", "iPhone"))
head(tweets)
#| fig-width: 8
#| fig-height: 8
data(stop_words)
tweets_iphone <- tweets %>%
filter(source = iPhone)
#| fig-width: 8
#| fig-height: 8
data(stop_words)
tweets_iphone <- tweets %>%
filter(source == iPhone)
#| fig-width: 8
#| fig-height: 8
data(stop_words)
tweets_iphone <- tweets %>%
filter(source == "iPhone")
tweets_android <- tweets %>%
filter(source == "Android")
frequencyTweets_iphone <- tweets_iphone %>%
unnest_tokens(output = word, input = text) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE)
frequencyTweets_android <- tweets_android %>%
unnest_tokens(output = word, input = text) %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE)
#We also don't want words like https and .co so let's filter them out
set.seed(231)
mypal <- brewer.pal(10, "Paired")
frequencyTweets_iphone %>%
slice(1:50) %>%
filter(word != "https") %>%
filter(word != "t.co") %>%
with(wordcloud(words = word,
freq = n,
min.freq = 20,
random.order = TRUE,
scale = c(4,0.3),
rot.per = 0.15,
colors = mypal,
family = "serif"))
set.seed(231)
mypal <- brewer.pal(10, "Paired")
frequencyTweets_android %>%
slice(1:50) %>%
filter(word != "https") %>%
filter(word != "t.co") %>%
with(wordcloud(words = word,
freq = n,
min.freq = 20,
random.order = TRUE,
scale = c(4,0.3),
rot.per = 0.15,
colors = mypal,
family = "serif"))
afinn_lexicon <- get_sentiments("afinn")
afinn_lexicon <- get_sentiments("afinn")
afinn_lexicon <- get_sentiments("afinn")
nrc_lexicon <- get_sentiments("nrc")
View(frequencyTweets)
View(tweets)
nrc_lexicon <- get_sentiments("nrc")
nrc_tweets <- tweets %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy", )) %>%
group_by()
nrc_lexicon <- get_sentiments("nrc")
nrc_tweets <- tweets %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy", )) %>%
group_by()
View(nrc_lexicon)
nrc_lexicon <- get_sentiments("nrc")
nrc_tweets <- frequencyTweets_iphone %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy", )) %>%
group_by()
nrc_lexicon <- get_sentiments("nrc")
nrc_tweets <- frequencyTweets_iphone %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy", )) %>%
arrange(sentiment, desc(n)) %>%
group_by(sentiment)
nrc_lexicon <- get_sentiments("nrc")
nrc_tweets <- frequencyTweets_iphone %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy" )) %>%
arrange(sentiment, desc(n)) %>%
group_by(sentiment)
nrc_lexicon <- get_sentiments("nrc")
nrc_tweets <- frequencyTweets_iphone %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy" )) %>%
arrange(sentiment, desc(n)) %>%
group_by(sentiment)
kable(nrc_tweets)
nrc_lexicon <- get_sentiments("nrc")
nrc_tweets <- frequencyTweets_iphone %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy" )) %>%
arrange(sentiment, desc(n)) %>%
group_by(n)
kable(nrc_tweets)
nrc_lexicon <- get_sentiments("nrc")
nrc_tweets <- frequencyTweets_iphone %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy" )) %>%
arrange(sentiment, desc(n))
kable(nrc_tweets)
nrc_lexicon <- get_sentiments("nrc")
nrc_tweets <- frequencyTweets_iphone %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy" )) %>%
arrange(sentiment, desc(n))
kable(nrc_tweets)
nrc_lexicon <- get_sentiments("nrc")
nrc_tweets <- frequencyTweets_iphone %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy" )) %>%
arrange(sentiment, desc(n)) %>%
summarize(
Sentiment = sentiment,
N = sum(n)
)
kable(nrc_tweets)
nrc_lexicon <- get_sentiments("nrc")
nrc_tweets <- frequencyTweets_iphone %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy" )) %>%
arrange(sentiment, desc(n))
kable(nrc_tweets)
nrc_lexicon <- get_sentiments("nrc")
nrc_tweets <- frequencyTweets_iphone %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy" )) %>%
arrange(desc(n))
kable(nrc_tweets)
nrc_lexicon <- get_sentiments("nrc")
nrc_tweetsiPhone <- frequencyTweets_iphone %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy" )) %>%
arrange(desc(n))
kable(nrc_tweetsiPhone, booktabs = TRUE)
nrc_tweetsAndroid <- frequencyTweets_android %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy" )) %>%
arrange(desc(n))
kable(nrc_tweetsAndroid, Booktabs = TRUE)
nrc_lexicon <- get_sentiments("nrc")
nrc_tweetsiPhone <- frequencyTweets_iphone %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy" )) %>%
arrange(desc(n)) %>%
summarize(
group_by(sentiment),
Number_of_words = sum(n)
)
nrc_lexicon <- get_sentiments("nrc")
nrc_tweetsiPhone <- frequencyTweets_iphone %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy" )) %>%
arrange(desc(n)) %>%
group_by(sentiment) %>%
summarize(
Sentiment = sentiment,
Number_of_words = sum(n)
)
kable(nrc_tweetsiPhone, booktabs = TRUE)
nrc_tweetsAndroid <- frequencyTweets_android %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy" )) %>%
arrange(desc(n))
kable(nrc_tweetsAndroid, Booktabs = TRUE)
nrc_lexicon <- get_sentiments("nrc")
nrc_tweetsiPhone <- frequencyTweets_iphone %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy" )) %>%
arrange(desc(n)) %>%
group_by(sentiment) %>%
reframe(
Sentiment = sentiment,
Number_of_words = sum(n)
)
kable(nrc_tweetsiPhone, booktabs = TRUE)
nrc_tweetsAndroid <- frequencyTweets_android %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy" )) %>%
arrange(desc(n))
kable(nrc_tweetsAndroid, Booktabs = TRUE)
nrc_lexicon <- get_sentiments("nrc")
nrc_tweetsiPhone <- frequencyTweets_iphone %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy" )) %>%
arrange(desc(n)) %>%
group_by(sentiment) %>%
reframe(
Number_of_words = sum(n)
)
kable(nrc_tweetsiPhone, booktabs = TRUE)
nrc_tweetsAndroid <- frequencyTweets_android %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy" )) %>%
arrange(desc(n))
kable(nrc_tweetsAndroid, Booktabs = TRUE)
nrc_lexicon <- get_sentiments("nrc")
nrc_tweetsiPhone <- frequencyTweets_iphone %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy" )) %>%
arrange(desc(n)) %>%
group_by(sentiment) %>%
reframe(
Number_of_words = sum(n)
)
kable(nrc_tweetsiPhone, booktabs = TRUE)
nrc_tweetsAndroid <- frequencyTweets_android %>%
inner_join(nrc_lexicon, by = "word") %>%
filter(sentiment %in% c("anger","joy" )) %>%
arrange(desc(n)) %>%
group_by(sentiment) %>%
reframe(
Number_of_words = sum(n)
)
kable(nrc_tweetsAndroid, Booktabs = TRUE)
